{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to work with spark's [structured streaming API](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) to stream a topic from confluent cloud.\n",
    "\n",
    "Based on confluent's [guide](https://www.confluent.io/blog/consume-avro-data-from-kafka-topics-and-secured-schema-registry-with-databricks-confluent-cloud-on-azure/#step-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-35a84c6f-eea0-49c6-a66e-4e705b4790a2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.4.1/spark-avro_2.12-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.4.1!spark-avro_2.12.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.1/spark-sql-kafka-0-10_2.12-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1!spark-sql-kafka-0-10_2.12.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.4.1/spark-token-provider-kafka-0-10_2.12-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1!spark-token-provider-kafka-0-10_2.12.jar (69ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.2/kafka-clients-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.3.2!kafka-clients.jar (477ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (65ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (77ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (2509ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (132ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.1/snappy-java-1.1.10.1.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.1!snappy-java.jar(bundle) (203ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.6/slf4j-api-2.0.6.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.6!slf4j-api.jar (59ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (1642ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (63ms)\n",
      ":: resolution report :: resolve 7570ms :: artifacts dl 5561ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-35a84c6f-eea0-49c6-a66e-4e705b4790a2\n",
      "\tconfs: [default]\n",
      "\t13 artifacts copied, 0 already retrieved (56754kB/86ms)\n",
      "23/08/15 14:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark connect does not support the structured streaming API\n",
    "\n",
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-avro_2.12:3.4.1\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\",\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"demo-structured-streaming-confluent\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(jar_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab some creds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluentClusterName = os.environ.get(\"CONFLUENT_CLUSTER_NAME\")\n",
    "confluentBootstrapServers = os.environ.get(\"CONFLUENT_BOOTSTRAP_SERVERS\")\n",
    "confluentTopicName = os.environ.get(\"CONFLUENT_TOPIC_NAME\")\n",
    "schemaRegistryUrl = os.environ.get(\"SCHEMA_REGISTRY_URL\")\n",
    "confluentApiKey = os.environ.get(\"CONFLUENT_API_KEY\")\n",
    "confluentSecret = os.environ.get(\"CONFLUENT_SECRET\")\n",
    "confluentRegistryApiKey = os.environ.get(\"CONFLUENT_REGISTRY_API_KEY\")\n",
    "confluentRegistrySecret = os.environ.get(\"CONFLUENT_REGISTRY_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BWyZxhTJsvSCk0UJWqzTzfet03d9L5AsTn+ju3X5epCrusxoUzIwISdZgHNlx6tK'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confluentSecret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schema registry client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_registry_conf = {\n",
    "    \"url\": schemaRegistryUrl,\n",
    "    \"basic.auth.user.info\": \"{}:{}\".format(\n",
    "        confluentRegistryApiKey, confluentRegistrySecret\n",
    "    ),\n",
    "}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the spark readstream (read from a kafka topic and do some data manipulation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Kafka message contains a key and a value. Data going through a Kafka topic in Confluent Cloud has five bytes added to the beginning of every Avro value. If you are using Avro format keys, then five bytes will be added to the beginning of those as well. For this example, we’re assuming string keys. These bytes consist of one magic byte and four bytes representing the schema ID of the schema in the registry that is needed to decode that data. The bytes need to be removed so that the schema ID can be determined and the Avro data can be parsed\n",
    "binary_to_string = fn.udf(\n",
    "    lambda x: str(int.from_bytes(x, byteorder=\"big\")), StringType()\n",
    ")\n",
    "\n",
    "\n",
    "clickstreamTestDf = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\n",
    "        \"kafka.sasl.jaas.config\",\n",
    "        \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
    "            confluentApiKey, confluentSecret\n",
    "        ),\n",
    "    )\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    "    .withColumn(\"key\", fn.col(\"key\").cast(StringType()))\n",
    "    .withColumn(\"fixedValue\", fn.expr(\"substring(value, 6, length(value)-5)\"))\n",
    "    .withColumn(\"valueSchemaId\", binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\n",
    "    .select(\n",
    "        \"topic\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"timestamp\",\n",
    "        \"timestampType\",\n",
    "        \"key\",\n",
    "        \"valueSchemaId\",\n",
    "        \"fixedValue\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./data/structured_streaming_confluent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAvroDataWithSchemaId(df, ephoch_id):\n",
    "    cachedDf = df.cache()\n",
    "    fromAvroOptions = {\"mode\": \"FAILFAST\"}\n",
    "\n",
    "    def getSchema(id):\n",
    "        return str(schema_registry_client.get_schema(id).schema_str)\n",
    "\n",
    "    distinctValueSchemaIdDF = cachedDf.select(\n",
    "        fn.col(\"valueSchemaId\").cast(\"integer\")\n",
    "    ).distinct()\n",
    "\n",
    "    for valueRow in distinctValueSchemaIdDF.collect():\n",
    "        currentValueSchemaId = sc.broadcast(valueRow.valueSchemaId)\n",
    "        currentValueSchema = sc.broadcast(getSchema(currentValueSchemaId.value))\n",
    "        filterValueDF = cachedDf.filter(\n",
    "            fn.col(\"valueSchemaId\") == currentValueSchemaId.value\n",
    "        )\n",
    "        filterValueDF.select(\n",
    "            \"topic\",\n",
    "            \"partition\",\n",
    "            \"offset\",\n",
    "            \"timestamp\",\n",
    "            \"timestampType\",\n",
    "            \"key\",\n",
    "            from_avro(\"fixedValue\", currentValueSchema.value, fromAvroOptions).alias(\n",
    "                \"parsedValue\"\n",
    "            ),\n",
    "        ).write.format(\"avro\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\n",
    "            DATA_FOLDER\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the write stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/15 14:39:22 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-99a58ebd-d47a-4cff-bd56-68de9ab8fb39. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/08/15 14:39:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "write_stream = (\n",
    "    clickstreamTestDf.writeStream.foreachBatch(parseAvroDataWithSchemaId)\n",
    "    .queryName(\"clickStreamTestFromConfluent\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the stream after 1 minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/15 14:39:23 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/15 14:39:28 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/15 14:39:28 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/15 14:39:28 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/15 14:39:28 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/15 14:39:28 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "23/08/15 14:39:28 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    }
   ],
   "source": [
    "time.sleep(60)\n",
    "write_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------+--------------------+-------------+---+--------------------+\n",
      "|       topic|partition|offset|           timestamp|timestampType|key|         parsedValue|\n",
      "+------------+---------+------+--------------------+-------------+---+--------------------+\n",
      "|clickstreams|        4|     0|2023-08-14 13:02:...|            0|  6|{6, AbdelKable_86...|\n",
      "|clickstreams|        4|     1|2023-08-14 13:02:...|            0| 10|{10, ArlyneW8ter,...|\n",
      "|clickstreams|        4|     2|2023-08-14 13:02:...|            0| 12|{12, Roberto_123,...|\n",
      "|clickstreams|        4|     3|2023-08-14 13:02:...|            0| 18|{18, alison_99, 1...|\n",
      "|clickstreams|        4|     4|2023-08-14 13:02:...|            0| 20|{20, ArlyneW8ter,...|\n",
      "+------------+---------+------+--------------------+-------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = spark.read.format(\"avro\").load(DATA_FOLDER)\n",
    "results_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
